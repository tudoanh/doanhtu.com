Title: What is mask attention?
Date: 04/15/22 15:50
Tags: machine learning, deep learning, python, pytorch
Authors: Do Anh Tu

[Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention)](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)

Under the hood, the model is composed of an <span class="underline">encoder</span> and a <span class="circle">decoder</span>.

<span class="bracket">The encoder processes each item in the input sequence, it compiles the information it captures into a vector (called the context). After processing the entire input sequence, the encoder sends the context over to the decoder, which begins producing the output sequence item by item.</span>

[A friendly introduction to Recurrent Neural Networks](https://www.youtube.com/watch?v=UNmqTiOnRfg)
<br>
